---
title: "Introductory Bayesian Course"
author: "Dr. Joseph L. Thorley"
date: "September 18^th^, 2014"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    keep_md: yes
    toc: yes
---

```{r,echo = FALSE, results='asis'}
library(knitr)
opts_chunk$set(tidy = TRUE)
options(width = 70)

exercise_number <- 0
exercise <- function (txt) {
  exercise_number <<- exercise_number + 1
  cat(paste0("\n**Exercise** ", exercise_number), paste0("*", txt, "*\n"))
}

ktable <- function (...) {
  x <- c(...)
  mat <- matrix(NA, nrow = length(x), ncol = 2)
  mat[, 1] <- names(x)
  mat[, 2] <- x
  colnames(mat) <- mat[1,]
  mat <- mat[-1, , drop = FALSE]
  mat <- mat[order(mat[, 1, drop = TRUE]), , drop = FALSE]
  cat("\n")
  cat(kable(mat, row.names = FALSE, output = FALSE), sep = "\n")
  cat("\n")
}

options(repos = c("CRAN" = "http://cran.rstudio.com"))

fig_width <- 4
fig_height <- fig_width

fn = local({
  i <- 0
  function(x) {
    i <<- i + 1
    paste0("Figure ", i, ": ", x)
  }
})

```

# Background

The purpose of these course notes is to introduce participants to Bayesian analysis with R, RStudio and JAGS. It is assumed that participants are familiar with R and RStudio as covered
in the Introductory R Course notes at <http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html>.

## Licence

The notes, which are released under a [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/deed.en_US) license, are a draft of the material to be presented at the [Introductory Bayesian Course](http://www.poissonconsulting.ca/courses/2014/04/02/bayesian-course.html) in Kelowna on November 20^th^-21^st^, 2014. They were written by [Dr. Joseph Thorley R.P.Bio.](http://www.poissonconsulting.ca/people/joe-thorley.html).

## Installation

If you haven't already done so, download the the most recent version of the R base distribution binary for your platform from <http://cran.r-project.org/> and install using the default options. Next download and install RStudio from <http://www.rstudio.com/products/rstudio/download/> using the default options. Then, download JAGS from <http://sourceforge.net/projects/mcmc-jags/files/JAGS/> and install with the default options. 

To make sure you have all the required packages execute
the following code at the command line
```{r, message = FALSE, warning = FALSE}
#install.packages("devtools", quiet = TRUE)
library(devtools)

#install.packages("dplyr", quiet = TRUE)
library(dplyr)

#install.packages("scales", quiet = TRUE)
library(scales)

#install.packages("ggplot2", quiet = TRUE)
library(ggplot2)

#install.packages("rjags", quiet = TRUE)
#install_github("poissonconsulting/tulip@v0.0.11")
#install_github("poissonconsulting/datalist@v0.4")
#install_github("poissonconsulting/juggler@v0.1.3")

#install_github("poissonconsulting/jaggernaut@v1.8.5")
library(jaggernaut)
```

## Bayesian vs. Frequentist Statistical Analysis

Statistical analysis uses probability models 
to provide bounded estimates of parameter values 
($\theta$) from the data ($y$).

There are two primary approaches to statistical analysis: Bayesian and frequentist. As far as a frequentist is concerned the best estimates of $\theta$ are those values that maximise the *likelihood* which is the probability of the data given the estimates, i.e., $p(y|\theta)$. 
A Bayesian on the other hand chooses the values with the highest *posterior* probability - that is to say the probability of the estimates given the data , i.e., $p(\theta|y)$. 

```{r, echo = FALSE, results = 'asis'}
exercise("Which criterion makes the most sense to you? And why?")
```

There are several major consequences of this choice. 

One major consequence is that Bayesians need to provide a *prior* probability distribution for $\theta$ because (as **Bayes** theorem demonstrates) $P(\theta|y) \propto P(y|\theta)P(\theta)$. Bayesians consider
this an advantage because prior information can be incorporated into an analysis while frequentists consider it [subjective](http://dilbert.com/strips/comic/2008-05-08/). In most cases Bayesians use *low-information* priors which have negligible effect on the posteriors.

A second major consequence is that frequentist 95% *confidence intervals* (CIs) are expected to include the actual value of $\theta$ 95% of the time. Bayesian 95% *credible intervals* (CRIs), on the other hand, have a 95% probability of including the actual
value of $\theta$. The difference is subtle but important. For example,
the reliability of frequentist confidence intervals depend on *sufficient*
sample size - Bayesian credible intervals do not.

A third major consequence is that although likelihood functions can be derived 
for many models (and therefore used to quickly find the maximum likelihood (ML) estimates),
it is typically not possible to to calculate the posterior probababilities.
As a result the posterior probabilities have to be sampled using Markov Chain Monte Carlo (MCMC) algorithms such as Gibbs Sampling.

## Gibbs Sampling

Consider the case where the parameters $\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ 
then Gibbs Sampling proceed as follows

Step 1

:   Choose starting *initial* values $\theta_1^{(0)}, \theta_2^{(0)}, \ldots, \theta_k^{(0)}$

Step 2

:   Sample $\theta_1^{(1)}$ from $p(\theta_1|\theta_2^{(0)},\theta_3^{(0)}, \ldots, \theta_k^{(0)}, y)$

    Sample $\theta_2^{(1)}$ from $p(\theta_2|\theta_1^{(1)},\theta_3^{(0)}, \ldots, \theta_k^{(0)}, y)$
    
    $\cdots$
    
    Sample $\theta_k^{(1)}$ from $p(\theta_k|\theta_1^{(1)},\theta_2^{(1)}, \ldots, \theta_{k-1}^{(1)}, y)$

Step 3

:   *Iterate* step 2 thousands (or millions) of times to obtain a sample from $p(\theta|y)$.

## JAGS and BUGS

Programming an efficient Gibbs Sampler for a particular model is outside the scope of 
most research projects. 
Fortunately, JAGS (which stands for Just Another Gibbs Sampler) can take a dataset
and a model specified in the simple but flexible BUGS language (which stands for Bayesian Analysis Using Gibbs Sampling) and perform iterative sampling for us. 

In order to do this we will use the `jaggernaut` package to talk to
the standalone JAGS program via the `rjags` package.

## Biased Coin

Now consider the example of a biased coin which has a 75% chance of giving a tail.
We can generate a dummy data set of 10 values 
using the R function `rbinom()` 
which provides 
[pseudo-random](http://search.dilbert.com/comic/Random%20Nine) 
samples from a binomial distribution.

```{r, fig.width=3, fig.height=3}
set.seed(563) # specifies start point for pseudo-random number generation
y <- rbinom(n = 10, size = 1, prob = 0.75) # generates vector of 1000 samples
y
mean(y) # calculates mean of vector
qplot(y, geom = "histogram") # plots histogram of vector
```

In order to estimate the underlying probability of throwing a head we specify the following model in the BUGS language and save it as an object of class `jags_model`.
```{r, tidy=FALSE}
mcoin1 <- jags_model("model { # start of BUGS model code
  theta ~ dunif(0, 1) # uniform prior distribution between 1 and 0 for theta
  for(i in 1:length(y)) { # for loop over the values 1, 2, ..., length of y
    y[i] ~ dbin(theta, 1) # assumed statistical distribution for observed throws
  } # end of for loop
} # end of BUGS model code
") 
```

```{r, echo = FALSE, results = 'asis'}
exercise("Plot theta's prior distribution as a histogram. You will likely find the `runif` and `qplot` functions useful.")
```

Then we call JAGS using `jaggernaut` in `debug` mode to generate 100 samples from 
$\theta$'s posterior probability distribution.

With these settings `jaggernaut` 

- generates two chains of 50 iterations in length

```{r, message=FALSE, warning=FALSE, results='hide'}
acoin1 <- jags_analysis(mcoin1, data = data.frame(y = y), mode = "debug")
```

```{r, fig.width=6, fig.height=3}
plot(acoin1)
coef(acoin1)
```

The model output indicates that the point estimate is `r round(coef(acoin1)[,"estimate"],2)`, the 95% credible interval is `r round(coef(acoin1)[,"lower"],2)` to `r round(coef(acoin1)[,"upper"],2)`, the standard deviation is `r round(coef(acoin1)[,"sd"],2)`,
the percent relative error is `r coef(acoin1)[,"error"]` and the significance is `r coef(acoin1)[,"significance"]`.


If previous studies indicated that the coin was biased towards tails then we could adjust the prior distribution as follows.
```{r, tidy=FALSE}
mcoin2 <- jags_model("model {
  theta ~ dunif(0.5, 1) # new prior distribution
  for(i in 1:length(y)) { 
    y[i] ~ dbin(theta, 1) 
  } 
} ")
```

```{r, echo = FALSE, results = 'asis'}
exercise("How does the new prior affect the posterior distribution?")
```

### Recap

- The BUGS model code defines the posterior distributions and relationships between
parameters.
- Based on the data and model definition, JAGS uses MCMC algorithms to iteratively sample from the posterior distributions of the monitored parameters.
- The posterior distributions can be summarised in terms of a point estimate and 95% credible intervals.


## Black Cherry Trees

The `trees` data set in the dataset package provides information on
the girth and volume of 31 black cherry trees.

```{r, fig.width=3, fig.height=3}
qplot(x = Girth, y = Volume, data = trees)
```

Algebraically, the linear regression of `Volume` against `Girth` can be defined as follows

$$Volume_{i}  = \alpha + \beta * Girth_{i} + \epsilon_{i}$$
  
  where $\alpha$ is the intercept and $\beta$ is the slope and the error terms ($\epsilon_{i}$) are  drawn from a normal distribution with an standard deviation of $\sigma$.


The model can be defined as follows in the BUGS language where
`<-` indicates a *deterministic* as 
opposed to *stochastic* node (which is indicated by `~`).

```{r, tidy=FALSE}
mtrees1 <- jags_model("model {
  alpha ~ dnorm(0, 50^-2) # normal prior on intercept
  beta ~ dnorm(0, 50^-2) # normal prior on slope
  sigma ~ dunif(0, 50) # positive uniform prior on residual variation

  for(i in 1:length(Volume)) { 
    eVolume[i] <- alpha + beta * Girth[i] # expected volume
    Volume[i] ~ dnorm(eVolume[i], sigma^-2) # observed volume drawn from normal
  } 
}")
```

The standard deviations of the normal distributions are raised to the
power of `-2` because (for historical reasons) Bayesians quantify variation 
in terms of the *precision* ($\tau$) as opposed to the variance ($\sigma^2$) or standard deviation ($\sigma$) where $\tau = 1/\sigma^2$.

The resultant trace plots and coefficients are as follows
```{r, message=FALSE, warning=FALSE, results='hide'}
atrees1 <- jags_analysis(mtrees1, data = datasets::trees, mode = "debug")
```

```{r, fig.width = 6, fig.height = 6}
plot(atrees1)
coef(atrees1)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What do the trace plots suggest to you?")
```

The $\hat{R}$ metric uses the within-chain and between-chain variances
to quantify the extent to which the chains have converged on the same distribution. Although a value of `1.0` indicates complete 
convergence, for most purposes an $\hat{R} < 1.1$ is considered sufficient.

The function `rhat` can return the $\hat{R}$ value for
each monitored parameter.

```{r}
rhat(atrees1, parm = "fixed", combine = FALSE)
```

Lack of convergence suggests that the MCMC samples may not be representative 
of the posterior distributions. To produce more representative samples,
the user can attempt to reparameterise the model to increase chain mixing
and/or increase the number of iterations.

### Chain Mixing

Poor chain mixing, which manifests as high levels of autocorrelation, is often caused by cross correlations
between parameters. Examination of the previous trace plots indicates
that `alpha` and `beta` are cross correlated. The following code sets the `select` terms of `mtrees1` model so that it specifies the required variables in the input
data with the `+` suffix on `Girth` indicating that it is to be
centered (`x - mean(x)`) before being passed to JAGS. 

```{r, tidy=FALSE}
select(mtrees1) <- c("Volume", "Girth+")
```


```{r, echo = FALSE, results = 'asis'}
exercise("Does centering `Girth` improve chain mixing?")
```

Note if you ever want to examine the actual data being passed
to JAGS set the modify_data term of your `jags_model` 
object to be a simple function that prints and returns its one argument 
```{r, tidy=FALSE}
modify_data(mtrees1) <- function (data) { print(data); data }
```

### Iterations

Convergence can also be achieved by increasing the number of
iterations.
This is easy to achieve using jaggernaut, just
switch to `report` mode and set `niters = 10^4`

```{r}
opts_jagr(mode = "report")
atrees3 <- jags_analysis(mtrees1, data = datasets::trees, niters = 10^4)
```

With these settings `jaggernaut` 

- undergoes an adaptive phase of xx iterations 
to maximize sampling efficiency (chain mixing)
- generates three chains of 10^4 iterations in length.
- discards the first half of each chain as burnin
- thins each chain so that the total number of MCMC samples
for each monitored parameter is as close as possible to but not less than 10^3
- tests for convergence under the criterion that $\hat{R} < 1.1$
- if convergence has not been achieved it discards the current
samples as burnin and doubles the number of iterations before
thinning again
- it repeats this up to three times or until convergence is achieved

For more information on the possible modes and options type `?opts_jagr`.

```{r, echo = FALSE, results = 'asis'}
exercise("How does switching to report mode with 10^3 iterations
         affect the trace plots with and without centering Girth?")
```

### Predictions and Residuals

Many researchers estimate fitted values, predictions and residuals by monitoring additional nodes in their model code. The disadvantages of
this approach are that: 

- the model code becomes more complicated. 
- the MCMC sampling takes longer.
- adding deried parameters requires a model rerun.
- the table of parameter estimates becomes unwiedly.

`jaggernaut` overcomes these problems by allowing 
derived parameters to be defined
in a separate chunk of BUGS code as demonstrated below.

```{r, tidy=FALSE, message=FALSE, warning=FALSE, results='hide'}
derived_code(mtrees1) <- "data {
  for(i in 1:length(Volume)) { 
    prediction[i] <- alpha + beta * Girth[i]
  }
  residual <- (Volume - prediction) / sigma
}"

atrees4 <- jags_analysis(mtrees1, data = datasets::trees, niters = 10^4)

prediction <- predict(atrees4, newdata = "Girth")
```

```{r, fig.width=3, fig.height=3, ,message=FALSE, warning=FALSE}
gp <- ggplot(data = prediction, aes(x = Girth, y = estimate))
gp <- gp + geom_point(data = dataset(atrees3), aes(y = Volume))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Volume")

print(gp)

residuals <- residuals(atrees4)
fitted <- fitted(atrees4)

qplot(fitted$estimate, residuals$estimate, xlab = "Fitted", ylab = "Residual") + geom_hline(yintercept = 0) + geom_smooth(se = FALSE)
```


As discussed in the R course [notes](http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html) the relationship between Volume and Girth is expected to be [allometric](http://www.nature.com/scitable/knowledge/library/allometry-the-study-of-biological-scaling-13228439) because the cross-sectional area at an given point scales to the square of the girth (circumference).

```{r, echo = FALSE, results = 'asis'}
exercise("Fit an allometric relationship by log transforming Volume and Girth within the model code (in this case do not center Girth). Does the residual plot suggest an improvement in the model?")

exercise("Does adding log transformed Height improve the model?")
```

The basic ANCOVA (analysis of covariance) model includes one categorical and one continous predictor variable without interactions. It can be expressed algebraically as follows

$$y_{ij}  = \alpha_{i} + \beta * x_{j} + \epsilon_{ij}$$

where $\alpha_{i}$ is the intercept for the i^th^ group mean and $\beta$ is the slope and the error terms ($\epsilon_{ij}$) are independently drawn from a normal distribution with standard deviation $\sigma$.

The following code fits the basic ANCOVA model to the ToothGrowth data and plots the model's predictions and residuals.

```{r, tidy=FALSE}
mtg1 <- jags_model("model {
  for(i in 1:nsupp) {
    alpha[i] ~ dnorm(0, 40^-2)
  }
  beta ~ dnorm(0, 20^-2)
  sigma ~ dunif(0, 20)

  for(i in 1:length(len)) { 
    eLen[i] <- alpha[supp[i]] + beta * dose[i]
    len[i] ~ dnorm(eLen[i], sigma^-2)
  } 
}",
derived_code  = " data{
  for(i in 1:length(len)) { 
    prediction[i] <- alpha[supp[i]] + beta * dose[i]
  }
  residual <- (len - prediction) / sigma
}",
select = c("len", "dose+", "supp"))
```

```{r, message=FALSE, warning=FALSE, results='hide'}
atg1 <- jags_analysis(mtg1, data = datasets::ToothGrowth)
```

```{r, fig.width = 4, fig.height = 3}
coef(atg1)

prediction <- predict(atg1, newdata = c("supp", "dose"))

gp <- ggplot(data = prediction, aes(x = dose, y = estimate, color = supp, shape = supp))
gp <- gp + geom_point(data = dataset(atg1), aes(y = len))
gp <- gp + geom_line()
gp <- gp + scale_y_continuous(name = "len")

print(gp)

residuals <- residuals(atg1)
residuals$fitted <- fitted(atg1)$estimate

qplot(fitted, estimate, color = supp, shape = supp, data = residuals, xlab = "Fitted", ylab = "Residual") + geom_hline(yintercept = 0)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What does the plot of the residual versus fitted values suggest about the model fit?")

exercise("Is the effect of OJ significantly different from that of VC?")

exercise("Modify the new ANCOVA model to fit 1) a linear regression for dose, 2) an ANOVA for supp and 3) an ANCOVA with an interaction between dose and supp. Which model do you prefer?")
```

### Effects Size

Often the results of an analysis are easier to understand when 
they are presented in terms of the percent change in the response.
The following code predicts and plots the percent change in `len` relative to 0.5 mg of VC for the ToothGrowth data set .

```{r, fig.width = 4, fig.height = 3}
prediction <- predict(atg1, newdata = c("supp", "dose"), base = data.frame(supp = "VC", dose = 0.5))

gp <- ggplot(data = prediction, aes(x = dose, y = estimate, color = supp, shape = supp))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Effect on len (%)", labels = percent)

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Plot the percent change in `len` for your preferred
         ToothGrowth model relative to 1 mg of OJ")
```

### House Martins and Hierarchical Models

Consider the annual surveys of house martins in a small Swiss village from 1990 to 2009 from Kery and Schaub (xxxx).

```{r, fig.width = 4, fig.height = 3}

data(hm)
qplot(year, hm, data = hm) + geom_line() + expand_limits(y = 0)
```

Algebraically a state-space population growth 
model to estimate the underlying population abundance $N_{t}$ 
and growth rate $r_{t}$ at time $t$ can be written 

$$log(N_{t+1}) = log(N_{t}) + r_{t}$$

$$r_{t} \sim N(\bar{r}, \sigma_{r})$$

$$log(Count_{t}) = log(N_{t}) + \epsilon_{t}$$

$$\epsilon_{t} \sim N(0, \sigma_{\epsilon})$$

In the BUGS language the model can be written as follows. 

```{r, tidy=FALSE}
mhm1 <- jags_model("model {
                   
  bar_r ~ dnorm(1, 10^-2)
  sigma_r ~ dunif(0, 1)
  sigma_e ~ dunif(0, 1)
  
  N[1] ~ dlnorm(5.6, 10^-2)
  for (i in 1:length(year)) {
    log(N[i+1]) <- log(N[i]) + r[i]
    r[i] ~ dnorm(bar_r, sigma_r^-2)
    hm[i] ~ dlnorm(log(N[i]), sigma_e^-2)
  }
}",
 derived_code = "data{
  for (i in 1:length(year)) {
    log(prediction[i]) <- log(N[year[i]])
  }
}",
random_effects = list(r = "year", N = "year"),
)
```

The specification of `r` and `N` as random effect means
that by default they are excluded from the trace plots 
and table of coefficients.

```{r, message=FALSE, warning=FALSE, results='hide'}
hm$year <- factor(hm$year)
ahm1 <- jags_analysis(mhm1, data = hm, niters = 10^4)
```

```{r, fig.width = 6, fig.height = 6}
plot(ahm1)
coef(ahm1)
```

```{r, fig.width = 3, fig.height = 3}
prediction <- predict(ahm1, newdata = "year")

gp <- ggplot(data = prediction, aes(x = as.integer(as.character(year)), y = estimate))
gp <- gp + geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 1/4)
gp <- gp + geom_line(data = dataset(ahm1), aes(y = hm), alpha = 1/3)
gp <- gp + geom_line()
gp <- gp + scale_x_continuous(name = "Year")
gp <- gp + scale_y_continuous(name = "Population Size")
gp <- gp + expand_limits(y = 0)

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What is the probability that the population in 2015 will be
         less than that in 2009? Note you can produce the projections
         by simply appending six years of missing counts to the dataset")
```

