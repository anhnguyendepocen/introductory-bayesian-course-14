---
title: "Introductory Bayesian Course"
author: "Dr. Joseph L. Thorley"
date: "October 20^th^, 2014"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    keep_md: yes
    toc: yes
bibliography: bibliography.bib
---

```{r,echo = FALSE, results='asis'}
library(knitr)
opts_chunk$set(tidy = TRUE)
options(width = 70)

exercise_number <- 0
exercise <- function (txt) {
  exercise_number <<- exercise_number + 1
  cat(paste0("\n**Exercise** ", exercise_number), paste0("*", txt, "*\n"))
}

ktable <- function (...) {
  x <- c(...)
  mat <- matrix(NA, nrow = length(x), ncol = 2)
  mat[, 1] <- names(x)
  mat[, 2] <- x
  colnames(mat) <- mat[1,]
  mat <- mat[-1, , drop = FALSE]
  mat <- mat[order(mat[, 1, drop = TRUE]), , drop = FALSE]
  cat("\n")
  cat(kable(mat, row.names = FALSE, output = FALSE), sep = "\n")
  cat("\n")
}

options(repos = c("CRAN" = "http://cran.rstudio.com"))

fig_width <- 4
fig_height <- fig_width

fn = local({
  i <- 0
  function(x) {
    i <<- i + 1
    paste0("Figure ", i, ": ", x)
  }
})

```

# Background

The purpose of these course notes is to introduce participants to Bayesian analysis with R, RStudio and JAGS. It is assumed that participants are familiar with R and RStudio as covered
in the Introductory R Course notes at <http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html>.

## Licence

The notes, which are released under a [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/deed.en_US) license, are a draft of the material to be presented at the [Introductory Bayesian Course](http://www.poissonconsulting.ca/courses/2014/04/02/bayesian-course.html) in Kelowna on November 20^th^-21^st^, 2014. They were written by [Dr. Joseph Thorley R.P.Bio.](http://www.poissonconsulting.ca/people/joe-thorley.html). The Rmd file to generate these notes is available
on [Github]<https://github.com/poissonconsulting/introductory-bayesian-course-14> together with
R/BUGS code to answer to the exercises.

## Installation

If you haven't already done so, download the the most recent version of the R base distribution binary for your platform from <http://cran.r-project.org/> and install using the default options. Next download and install RStudio from <http://www.rstudio.com/products/rstudio/download/> using the default options. Then, download JAGS from <http://sourceforge.net/projects/mcmc-jags/files/JAGS/> and install with the default options. If you are using Windows please install
the latest version of Rtools from <http://cran.r-project.org/bin/windows/Rtools/>. And for OSX users
make sure you have the latest version of Xcode <https://developer.apple.com/xcode/>. 

To install all the required packages execute the following code at the R console.

```{r, eval = FALSE}
install.packages("devtools", quiet = TRUE)
install.packages("dplyr", quiet = TRUE)
install.packages("ggplot2", quiet = TRUE)
install.packages("scales", quiet = TRUE)

library(devtools)
install_github("poissonconsulting/tulip@v0.0.11")
install_github("poissonconsulting/datalist@v0.4")
install_github("poissonconsulting/juggler@v0.1.3")
install_github("poissonconsulting/jaggernaut@v2.2.2")
```

If everything was successful, when you run the following code
```{r, eval=FALSE, tidy=FALSE}
library(jaggernaut)
model <- jags_model("model {
 bLambda ~ dlnorm(0,10^-2)
 for (i in 1:length(x)) {
   x[i]~dpois(bLambda)
 }
}")

summary(jags_analysis (model, data = data.frame(x = rpois(100,1))))
```
you should see something like
```
Analysis converged (rhat:1)

Model1:

Dimensions:
samples  chains 
   1500       3 

Convergence:
rhat 
   1 

Estimates:
         estimate     lower    upper       sd error significance
bLambda 0.8771658 0.7151765 1.062476 0.088936    20            0
```

## R Script Headers

During the course you should begin R scripts with 
```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(scales)
library(jaggernaut)
```
to load the required packages in the search path, and 
```{r, eval = FALSE}
graphics.off()
rm(list = ls())
```
to clean up the workspace and close any graphics windows.

# Bayesian and Frequentist Statistical Analysis

Statistical analysis uses probability models 
to provide bounded estimates of parameter values 
($\theta$) from the data ($y$).

There are two primary approaches to statistical analysis: Bayesian and frequentist. As far as a frequentist is concerned the best estimates of $\theta$ are those values that maximise the *likelihood* which is the probability of the data given the estimates, i.e., $p(y|\theta)$. 
A Bayesian on the other hand chooses the values with the highest *posterior* probability - that is to say the probability of the estimates given the data , i.e., $p(\theta|y)$.

## Coin Flips

Consider the case where $n = 10$ flips of a coin produce $y = 3$ tails.
We can model this using a binomial distribution

$$y \sim dbin(\theta, n)$$

where $\theta$ is the probability of throwing a head.

### Maximum Likelihood

The likelihood for the binomial model is given by the following equation

$$ p(y|\theta) =  {n \choose y} \theta^{y}(1-\theta)^{n-y}$$.

The likelihood values for different values of $\theta$ are
therefore as follows

```{r, fig.width = 3, fig.height = 3}
likelihood <- function (theta, n = 10, y = 3) {
  choose(n, y) * theta^y * (1 - theta)^(n-y)
}
theta <- seq(from = 0, to = 1, length.out = 100)

qplot(theta, likelihood(theta), geom = "line", ylab = "Likelihood", xlab = "theta")
```

The frequentist point estimate ($\hat{\theta}$) is the value of $\theta$ with the maximum likelihood (ML) value, which in this case is 0.3.

A 95% confidence interval (CI) can then be calculated using the asymptotic normal approximation

$$\hat{\theta} \pm 1.96 \frac{1}{\sqrt{I(\hat{\theta})}}$$

where $I(\hat{\theta})$ is the expected second derivative 
of the log-likelihood at the estimate. This calculation is 
based on the assumption that the sample size is of sufficient
size that the likelihood is normally distributed.

In the current case, 

$$I(\hat{\theta}) = \frac{n}{\hat{\theta}(1 - \hat{\theta})}$$

which gives a point estimate of 0.3 and lower and upper
95% confidence intervals of `r round(0.3 - 1.96 * (1/sqrt(10/0.21)),2)`
and `r round(0.3 + 1.96 * (1/sqrt(10/0.21)),2)` respectively.

### Posterior Probability

The posterior probability on the other hand 
is given by Bayes rule which states that

$$p(\theta|y) \propto p(y|\theta)p(\theta)$$

where $p(\theta)$ is the prior probability.

Bayesians consider the necessity to define prior probabilities an advantage 
because prior information can be incorporated into an analysis while 
frequentists consider it [subjective](http://dilbert.com/strips/comic/2008-05-08/). 
In most cases Bayesians use *low-information* priors which have little influence on the posteriors. 
For example a uniform distribution with a lower limit of 0 and an upper limit of 1 ($dunif(0,1)$) is commonly used for probabilities.

```{r, fig.width = 3, fig.height = 3}
qplot(runif(10^6, 0, 1), geom = "density", xlab = "theta")
```

As it is generally not possible to calculate the 
posterior probability, the  posterior probability distribution is sampled 
using Markov Chain Monte Carlo (MCMC) algorithms such as Gibbs Sampling.

#### Gibbs Sampling

Consider the case where the parameters 
$\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ 
then Gibbs Sampling proceed as follows

Step 1

:   Choose starting *initial* values for $\theta_1^{(0)}$ and $\theta_2^{(0)}$

Step 2

:   Sample $\theta_1^{(1)}$ from $p(\theta_1|\theta_2^{(0)}, y)$

    Sample $\theta_2^{(1)}$ from $p(\theta_2|\theta_1^{(1)}, y)$

Step 3

:   *Iterate* step 2 thousands (or millions) of times to obtain a sample from $p(\theta|y)$.

Typically this is performed for two or more independent chains.

## JAGS and BUGS

Programming an efficient MCMC algorithm
for a particular model is outside the scope of 
most research projects. 
Fortunately, JAGS (which stands for Just Another Gibbs Sampler) can take a dataset
and a model specified in the simple but flexible BUGS language (which stands for Bayesian Analysis Using Gibbs Sampling) and perform MCMC sampling for us. 

In order to do this we will use the `jaggernaut` package to talk to
the standalone JAGS program via the `rjags` package.

First we need to specify the underlying probability model in the BUGS language and save it as an object of class `jags_model`.
```{r, tidy=FALSE}
model1 <- jags_model("model { 
  theta ~ dunif(0, 1)
  y ~ dbin(theta, n)
}") 
```

then we call JAGS using `jaggernaut` in the default report mode 
to generate a total of $\geq 10^3$ samples using three chains from $\theta$'s posterior probability distribution.

```{r, message=FALSE, warning=FALSE, results='hide'}
data <- data.frame(n = 10, y = 3)
analysis1 <- jags_analysis(model1, data = data)
```

```{r, fig.width=6, fig.height=3}
plot(analysis1)
coef(analysis1)
```

The model output indicates that the point estimate (in this case the
mean of the samples) is `r round(coef(analysis1)[,"estimate"],2)`
and the 95% credible interval (in this case the 2.25th and 97.75th 
percentiles) is `r round(coef(analysis1)[,"lower"],2)` to `r round(coef(analysis1)[,"upper"],2)`. The model output also indicates that the 
posterior probability distribution
has a standard deviation (sd) of `r round(coef(analysis1)[,"sd"],2)`.
The significance and error values are discussed later.

```{r, echo = FALSE, results = 'asis'}
exercise("Previous studies indicate that the coin was definitely biased
         towards tails. Modify the prior distribution accordingly
         and rerun the above model. How does the posterior distribution change?")
```

# Black Cherry Trees

The `trees` data set in the dataset package provides information on
the girth and volume of 31 black cherry trees.

```{r, fig.width=3, fig.height=3}
qplot(x = Girth, y = Volume, data = trees)
```

Algebraically, the linear regression of `Volume` against `Girth` can be defined as follows

$$Volume_{i}  = \alpha + \beta * Girth_{i} + \epsilon_{i}$$
  
  where $\alpha$ is the intercept and $\beta$ is the slope and the error terms ($\epsilon_{i}$) are 
  independently drawn from a normal distribution with an standard deviation of $\sigma$.


The model can be defined as follows in the BUGS language where
`<-` indicates a *deterministic* as 
opposed to *stochastic* node (which is indicated by `~`).

```{r, tidy=FALSE}
model1 <- jags_model("model {
  alpha ~ dnorm(0, 50^-2) 
  beta ~ dnorm(0, 10^-2)
  sigma ~ dunif(0, 10)

  for(i in 1:length(Volume)) { 
    eMu[i] <- alpha + beta * Girth[i]
    Volume[i] ~ dnorm(eMu[i], sigma^-2)
  } 
}")
```

The standard deviations of the normal distributions are raised to the
power of `-2` because (for historical reasons) Bayesians quantify variation 
in terms of the *precision* ($\tau$) as opposed to the variance ($\sigma^2$) or standard deviation ($\sigma$) where $\tau = 1/\sigma^2$.

## Parallel Chains

To reduce the analysis time, 
MCMC chains can be run on parallel processes. In `jaggernaut` this
achieved using the `registerDoParallel` function
and by setting the parallel option to `TRUE`.
This only needs to be done once at the start of a session.
I add it to my R script header.
```{r}
if (getDoParWorkers() == 1) {
  registerDoParallel(3)
  opts_jagr(parallel = TRUE)
}
```

The resultant trace plots and coefficients for the `trees` analysis
are as follows.
```{r, message=FALSE, warning=FALSE, results='hide'}
data(trees)
analysis1 <- jags_analysis(model1, data = trees)
```

```{r, fig.width = 6, fig.height = 6}
plot(analysis1)
coef(analysis1)
```

```{r, echo=FALSE, results='hide'}
auto_corr(analysis1)
cross_corr(analysis1)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What do you notice about the trace plots? The output of 
         `auto_corr(analysis1)` and `cross_cor(analysis1)` 
         might give you some clues.")
```

## Convergence 

The $\hat{R}$ convergence metric uses the within-chain and between-chain variances
to quantify the extent to which the chains have converged on the same distribution. 
Although a value of `1.0` indicates full 
convergence, $\hat{R} \leq 1.05$ is typically 
considered sufficient for most papers and $\leq 1.10$ for most reports.

```{r, echo = FALSE, results = 'asis'}
exercise("What is the R-hat value for each of the parameters in the current model? 
         Use the `convergence` function with `combine = FALSE`.")
```

Lack of convergence suggests that the MCMC samples may not be representative 
of the posterior distributions.

## Iterations

Convergence can often be improved by simply increasing the number of
iterations.

```{r, message=FALSE, warning=FALSE, results='hide'}
analysis1 <- jags_analysis(model1, data = trees, niters = 10^4)
```

```{r, fig.width = 6, fig.height = 6}
plot(analysis1)
```

## Modes

In the default `opts_jagr()` `report` mode 

- an adaptive phase of 100 iterations is undertaken to maximize sampling efficiency (chain mixing)
- three chains of `niters` iterations in length are generated.
- the first half of each chain is discarded as burn in.
- each chain os thinned so that the total number of MCMC samples
for each monitored parameter is $\geq 10^3$.
- convergence is tested under the criterion that $\hat{R} < 1.1$
- if convergence has not been achieved the current
samples are discarded as burn in and the number of iterations doubled before
thinning again.
- this is repeated up to three times or until the 
convergence threshold of 1.1 is achieved.

To see the current mode and option settings type `opts_jagr()` and
for more information type `?opts_jagr`.

```{r, echo = FALSE, results = 'asis'}
exercise("Call `jags_analysis` with `niters = 10^2` and `mode = 'paper'`
         How many updates are required for convergence to be achieved?")
```

## Chain Mixing

Cross-correlations between parameters, which cause poor chain mixing (i.e., high auto-correlation and poor
convergence), can sometimes be eliminated or reduced by 
reparameterising the model. 
In the current model, `Girth` can be centered, i.e., `Girth - mean(Girth)`, in the BUGS code or by setting the `select_data` argument to be `select_data(model1) <- c("Volume", "Girth+")`.

```{r, echo = FALSE, results = 'asis'}
exercise("What is the effect of centering `Girth` on the trace plots?")
```

Note if you ever want to examine the actual data being passed
to JAGS set the modify_data term of your `jags_model` 
object to be a simple function that prints and returns its one argument 
```{r, tidy=FALSE}
modify_data(model1) <- function (data) { print(data); data }
```

## Derived Parameters

Many researchers estimate fitted values and residuals, generate predictions and
perform posterior predictive checks by monitoring additional nodes in their model code. 

The disadvantages of this approach are that: 

- the model code becomes cluttered. 
- the MCMC sampling takes longer.
- adding derived parameters requires a model rerun.
- the table of parameter estimates becomes unwieldy.

`jaggernaut` overcomes these problems by allowing 
derived parameters to be defined
in a separate chunk of BUGS code as demonstrated below.

```{r, tidy=FALSE, message=FALSE, warning=FALSE, results='hide'}
derived_code <- "data {
  for(i in 1:length(Volume)) { 
    prediction[i] <- alpha + beta * Girth[i]

    simulated[i] ~ dnorm(prediction[i], sigma^-2)

    D_observed[i] <- log(dnorm(Volume[i], prediction[i], sigma^-2))
    D_simulated[i] <- log(dnorm(simulated[i], prediction[i], sigma^-2))
  }
  residual <- (Volume - prediction) / sigma
  discrepancy <- sum(D_observed) - sum(D_simulated)
}"
```

### Predictions

To better understand a model and/or inform management decisions it is 
generally useful to plot a model's predictions.

In the following example, the predict function is used to
estimate the `Volume` with 95% CRIs and 95% Prediction Intervals (PRIs) 
across the range of the observed values of `Girth`.

```{r, tidy=FALSE, message=FALSE, warning=FALSE, results='hide'}
prediction <- predict(analysis1, newdata = "Girth", derived_code = derived_code)
simulated <- predict(analysis1, parm = "simulated", newdata = "Girth", derived_code = derived_code)
```

```{r, fig.width=3, fig.height=3, ,message=FALSE, warning=FALSE}
gp <- ggplot(data = prediction, aes(x = Girth, y = estimate))
gp <- gp + geom_point(data = dataset(analysis1), aes(y = Volume))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + geom_line(data = simulated, aes(y = lower), linetype = "dotted")
gp <- gp + geom_line(data = simulated, aes(y = upper), linetype = "dotted")
gp <- gp + scale_y_continuous(name = "Volume")

print(gp)

```

```{r, echo = FALSE, results = 'asis'}
exercise("The `newdata` argument in the `predict` function can also take a `data.frame`.
         What is the 95% Prediction Interval for the `Volume` of a tree of `Girth` 8?")
```

### Residuals

The model assumes that the error terms are independently drawn from a normal distribution.
It is possible to assess the adequacy of this assumption by plotting the residual variation.

```{r, fig.width=3, fig.height=3, message=FALSE, warning=FALSE}

fitted <- fitted(analysis1, derived_code = derived_code)
fitted$residual <- residuals(analysis1, derived_code = derived_code)$estimate

qplot(estimate, residual, data = fitted) + geom_hline(yintercept = 0) + geom_smooth(se = FALSE)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What does the current residual plot suggest to you about model adequacy?")
```

### Posterior Predictive Checks

A complementary approach to assessing model adequacy is to simulate data
given the model parameters and compare it to the observed data. Any systematic
differences indicate potential failings of the model.

```{r, fig.width=3, fig.height=3, message=FALSE, warning=FALSE}
predictive_check(analysis1, derived_code = derived_code)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What does the posterior predictive check suggest to you about model adequacy?")
```

## Allometry

As discussed in the R course [notes](http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html) the relationship between `Volume` and `Girth` is expected to be [allometric](http://www.nature.com/scitable/knowledge/library/allometry-the-study-of-biological-scaling-13228439) because the cross-sectional area at an given point scales to the square of the girth (circumference).

Expressed as an allometric relationship the model becomes

$$Volume_{i}  = \alpha * Girth_{i}^\beta * \epsilon_{i}$$

which can be reparameterised as a linear regression by log transforming
`Volume` and `Girth`.

$$log(Volume_{i})  = \alpha + \beta * log(Girth_{i}) + \epsilon_{i}$$

Variables can be log transformed in the model code or in the `select_data`
argument, i.e., `select_data(model1) <- c("log(Volume)", "log(Girth)")`.

```{r, echo = FALSE, results = 'asis'}
exercise("Fit the linear regression of the allometric model to the `trees` data set.
         Is the model fit improved?")
exercise("Is there any support for adding `log(Height)` to the model?")
```

## Significance Values

The significance value in the jaggernaut table of coefficients
is twice the probability that the posterior distribution spans
zero. As such it represents the Bayesian equivalent of a 
frequentist two-sided p-value [@greenland_living_2013].
By definition, parameters that represent standard deviations will have a significance value
of 0 (as they
must be greater than zero).

## Error Values

The error value in the table of coefficients is
the *percent relative error*,  which is half the credible interval
as a percent of the point estimate, i.e.,

$$error = (upper - lower) * 0.5 / estimate * 100$$.

Standard deviations with a uniform prior distribution that
is not updated by the data have error values of 0.95.
As a general rule, I question the informativeness
of parameters representing  standard deviations 
which have an error value $\geq 0.8$.

# Tooth Growth

The basic ANCOVA (analysis of covariance) model includes one categorical and one continous predictor variable without interactions. It can be expressed algebraically as follows

$$y_{ij}  = \alpha_{i} + \beta * x_{j} + \epsilon_{ij}$$

where $\alpha_{i}$ is the intercept for the i^th^ group mean and $\beta$ is the slope and the error terms ($\epsilon_{ij}$) are independently drawn from a normal distribution with standard deviation $\sigma$.

The following code fits the basic ANCOVA model to the ToothGrowth data and plots the model's predictions and residuals.

```{r, tidy=FALSE}
model1 <- jags_model("model {
  for(i in 1:nsupp) {
    alpha[i] ~ dnorm(0, 40^-2)
  }
  beta ~ dnorm(0, 20^-2)
  sigma ~ dunif(0, 20)

  for(i in 1:length(len)) { 
    eLen[i] <- alpha[supp[i]] + beta * dose[i]
    len[i] ~ dnorm(eLen[i], sigma^-2)
  } 
}",
derived_code  = " data{
  for(i in 1:length(len)) { 
    prediction[i] <- alpha[supp[i]] + beta * dose[i]
  }
  residual <- (len - prediction) / sigma
}")
```

```{r, message=FALSE, warning=FALSE, results='hide'}
data(ToothGrowth)
analysis1 <- jags_analysis(model1, data = ToothGrowth)
```

```{r, fig.width = 4, fig.height = 3}
coef(analysis1)

prediction <- predict(analysis1, newdata = c("supp", "dose"))

gp <- ggplot(data = prediction, aes(x = dose, y = estimate, color = supp, shape = supp))
gp <- gp + geom_point(data = dataset(analysis1), aes(y = len))
gp <- gp + geom_line()
gp <- gp + scale_y_continuous(name = "len")

print(gp)

residuals <- residuals(analysis1)
residuals$fitted <- fitted(analysis1)$estimate

qplot(fitted, estimate, color = supp, shape = supp, data = residuals, xlab = "Fitted", ylab = "Residual") + geom_hline(yintercept = 0)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Is the effect of `OJ` significantly different from that of `VC`?")

exercise("What does the residual plot suggest about the model fit?")
```

## Multiple Models

The linear regression on `dose` is given by 

```{r, tidy=FALSE}
model2 <- jags_model("model {
  alpha ~ dnorm(0, 40^-2)
  beta ~ dnorm(0, 20^-2)
  sigma ~ dunif(0, 20)

  for(i in 1:length(len)) { 
    eLen[i] <- alpha + beta * dose[i]
    len[i] ~ dnorm(eLen[i], sigma^-2)
  } 
}",
derived_code  = " data{
  for(i in 1:length(len)) { 
    prediction[i] <- alpha + beta * dose[i]
  }
  residual <- (len - prediction) / sigma
}",
select_data = c("len", "dose"))
```

The `combine` function allows multiple `jags_model` objects which can have
unique `mode_id`'s to be combined into a single object.

```{r, tidy=FALSE, message=FALSE, warning=FALSE, results='hide'}
model_id(model1) <- "ANCOVA"
model_id(model2) <- "regression"

models <- combine(model1, model2)
analyses <- jags_analysis(models, data = datasets::ToothGrowth)
```

```{r, fig.width = 4, fig.height = 3}
coef(analyses)

prediction <- predict(analyses, newdata = c("supp", "dose"), model_id = "regression")

gp <- ggplot(data = prediction, aes(x = dose, y = estimate, color = supp, shape = supp))
gp <- gp + geom_point(data = dataset(analyses), aes(y = len))
gp <- gp + geom_line()
gp <- gp + scale_y_continuous(name = "len")

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Fit 1) the ANCOVA, 2) the linear regression, 3) the ANOVA and 4) the ANCOVA with an interaction between dose and supp models and plot their predictions. Which model do you prefer?")
```

### Effects Size

Often the results of an analysis are easier to understand when 
they are presented in terms of the percent change in the response.
The following code predicts and plots the percent change in `len` 
relative to 0.75 mg of Vitamin C.

```{r, fig.width = 4, fig.height = 3}
prediction <- predict(analysis1, newdata = c("supp", "dose"), base = data.frame(supp = "OJ", dose = 0.75))

gp <- ggplot(data = prediction, aes(x = dose, y = estimate, color = supp, shape = supp))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Effect on len (%)", labels = percent)

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Plot the percent change in `len` for all four models relative to 0.5 mg of Vitamin C")
```

# Peregrine Falcon Population

Consider the peregrine falcon population data `?peregrine`.
the following code regresses the number of reproductive `Pairs` on `Year`.

```{r, tidy=FALSE}
model1 <- jags_model("model {
  alpha ~ dnorm(0, 100^-2)
  beta ~ dnorm(0, 100^-2)
  sigma ~ dunif(0, 100)
  for(i in 1:length(Pairs)) {
    ePairs[i] <- alpha + beta * Year[i]
    Pairs[i] ~ dnorm(ePairs[i], sigma^-2)
  }
}",
derived_code = "data {
  for(i in 1:length(Pairs)) {
    prediction[i] <- alpha + beta * Year[i]
  }
}",
select_data = c("Pairs", "Year+"))
```

```{r, message=FALSE, warning=FALSE, results='hide'}
data(peregrine)

analysis1 <- jags_analysis(model1, data = peregrine)
```

```{r}
coef(analysis1)

prediction <- predict(analysis1)
```

```{r, fig.width = 3, fig.height = 3}
gp <- ggplot(data = prediction, aes(x = Year, y = estimate))
gp <- gp + geom_point(data = dataset(analysis1), aes(y = Pairs))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Pairs")
gp <- gp + expand_limits(y = 0)

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Do you consider the model to be adequate?")
```

## Log-link Function

When a response variable cannot take negative values, the use of a log-link function ensures that 
the expected value must be positive.

```{r, echo = FALSE, results = 'asis'}
exercise("Replace `ePairs[i] <- ` with `log(ePairs[i]) <- ...`. How does the log-link function alter the model?")
```

## Poisson Distribution

The Poisson distribution models counts about a positive mean expected value. 
It can only assume discrete non-negative values and has a variance equal to the mean.

```{r, fig.width = 3, fig.height = 3}
qplot(rpois(1000, 1), geom = "histogram", binwidth = 1, color = I("white"), xlim = c(0, 30))
qplot(rpois(1000, 10), geom = "histogram", binwidth = 1, color = I("white"), xlim = c(0, 30)) 
```

```{r, echo = FALSE, results = 'asis'}
exercise("Next, replace `Pairs[i] ~ dnorm(ePairs[i], sigma^-2)` with `Pairs[i] ~ dpois(ePairs[i])`.
         How does the assumption of Poisson distributed counts alter the model?")
```

## Polynomial Regression

Curvature in a predictor variable can be modelled by allowing the influence of 
a variable to vary with the variable. For example, the following model code
fits a second-order polynomial on `Year`.

```{r, tidy = FALSE}
model_code <- "model {
  alpha ~ dnorm(0, 100^-2)
  beta ~ dnorm(0, 100^-2)
  beta2 ~ dnorm(0, 100^-2)
  for(i in 1:length(Pairs)) {
    log(ePairs[i]) <- alpha + beta * Year[i]  + beta2 * Year[i]^2
    Pairs[i] ~ dpois(ePairs[i])
  }
}"
```

```{r, echo = FALSE, results = 'asis'}
exercise("Does it improve the model?")
exercise("Fit a third-order polynomial (`beta3 * Year[i]^3`). How does it improve the model?")
exercise("Use the third-order polynomial model to predict the number of breeding pairs in 2006.
         How confident are you in your answer?")
```

## State-Space Population Growth Model

An alternative to a polynomial would be to fit a state-space
population growth model which explicity estimates
the step changes in the underlying population

$$log(N_{t+1}) = log(N_{t}) + r_{t}$$
$$r_{t} \sim dnorm(\bar{r}, \sigma_{r})$$


```{r, tidy=FALSE}
model6 <- jags_model("model {
  mean_r ~ dnorm(0, 1^-2)
  sd_r ~ dunif(0, 1)
  
  logN1 ~ dnorm(0, 10^-2)
  logN[1] <- logN1
  for(i in 2:nYear) {
    logN[i] <- logN[i-1] + r[i-1]
  }
  
  for(i in 1:nYear) {
    r[i] ~ dnorm(mean_r, sd_r^-2)
    Pairs[i] ~ dpois(exp(logN[i]))
  }
}",
derived_code = "data {
  for(i in 1:length(Pairs)) {
    log(prediction[i]) <- logN[Year[i]]
  }
}",
modify_data = function (data) {
  stopifnot(!is.unsorted(data$Year))
  data
},
select_data = c("Pairs", "Year"),
random_effects = list(r = "Year", logN = "Year"))
```

The specification of `r` and `logN` as random effect means
that by default they are excluded from the trace plots 
and table of coefficients.

```{r, message=FALSE, warning=FALSE, results='hide'}
peregrine$Year <- factor(peregrine$Year)
analysis6 <- jags_analysis(model6, data = peregrine)
```

```{r, fig.width = 6, fig.height = 6}
plot(analysis6)
coef(analysis6)
```

```{r, fig.width = 3, fig.height = 3}
prediction <- predict(analysis6)

gp <- ggplot(data = prediction, aes(x = as.integer(as.character(Year)), y = estimate))
gp <- gp + geom_point(data = dataset(analysis6), aes(y = Pairs))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_x_continuous(name = "Year")
gp <- gp + scale_y_continuous(name = "Pairs")
gp <- gp + expand_limits(y = 0)

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Plot the state-space population growth rate model predictions in terms 
         of the percent change since 1970. What is the estimated percent
         change in the population in 2003 compared to 1970?")
exercise("What is the probability that the population in 2008 will be
         less than that in 2003? Note you can produce the projections
         by simply appending five years of missing counts to the dataset.")
```

## Logistic Model

Now let us consider the number of `Pairs` that successfully reproduced (`R.pairs`).

```{r, fig.width = 3, fig.height = 3}
data(peregrine)

peregrine$Proportion <- peregrine$R.Pairs / peregrine$Pairs

model7 <- jags_model("model {
  alpha ~ dnorm(0, 1^-2)
  beta ~ dnorm(0, 1^-2)
  sigma ~ dunif(0, 1)
  for(i in 1:length(Proportion)) {
    eProportion[i] <- alpha + beta * Year[i]
    Proportion[i] ~ dnorm(eProportion[i], sigma^-2)
  }
}",
derived_code = "data {
  for(i in 1:length(Proportion)) {
    prediction[i] <- alpha + beta * Year[i]
  }
}",
select_data = c("Proportion", "Year+"))
```

```{r, message=FALSE, warning=FALSE, results='hide'}
analysis7 <- jags_analysis(model7, data = peregrine)
```

```{r, fig.width = 3, fig.height = 3}
prediction <- predict(analysis7)

gp <- ggplot(data = prediction, aes(x = Year, y = estimate))
gp <- gp + geom_point(data = dataset(analysis7), aes(y = Proportion))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_x_continuous(name = "Year")
gp <- gp + scale_y_continuous(name = "Pairs")
gp <- gp + expand_limits(y = c(0,1))

print(gp)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Fit a second-order polynomial regression to the proportion of successful pairs")
exercise("Add a logistic-link function, i.e., logit(eProportion[i]) <- ")
exercise("Replace the normal distribution `Proportion[i] ~ dnorm(eProportion[i], sigma^-2)` 
         with the binomial distribution `R.Pairs ~ dbin(eProportion[i], Pairs[i])`")
exercise("Replace the polynomial with a first-order difference auto-regressive term")
exercise("Add extra-binomial variation through a normally distributed random effect on 
         `logit(eProportion[i])`")
```

# References

