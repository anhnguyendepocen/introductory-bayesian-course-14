---
title: "Introductory Bayesian Course"
author: "Dr. Joseph L. Thorley"
date: "September 18^th^, 2014"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    keep_md: yes
    toc: yes
---

```{r,echo = FALSE, results='asis'}
library(knitr)
opts_chunk$set(tidy = TRUE)
options(width = 70)

exercise_number <- 0
exercise <- function (txt) {
  exercise_number <<- exercise_number + 1
  cat(paste0("\n**Exercise** ", exercise_number), paste0("*", txt, "*\n"))
}

ktable <- function (...) {
  x <- c(...)
  mat <- matrix(NA, nrow = length(x), ncol = 2)
  mat[, 1] <- names(x)
  mat[, 2] <- x
  colnames(mat) <- mat[1,]
  mat <- mat[-1, , drop = FALSE]
  mat <- mat[order(mat[, 1, drop = TRUE]), , drop = FALSE]
  cat("\n")
  cat(kable(mat, row.names = FALSE, output = FALSE), sep = "\n")
  cat("\n")
}

options(repos = c("CRAN" = "http://cran.rstudio.com"))

fig_width <- 4
fig_height <- fig_width

fn = local({
  i <- 0
  function(x) {
    i <<- i + 1
    paste0("Figure ", i, ": ", x)
  }
})

```

# Background

The purpose of these course notes is to introduce participants to Bayesian analysis with R, RStudio and JAGS. It is assumed that participants are familiar with R and RStudio as covered
in the Introductory R Course notes at <http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html>.

## Licence

The notes, which are released under a [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/deed.en_US) license, are a draft of the material to be presented at the [Introductory Bayesian Course](http://www.poissonconsulting.ca/courses/2014/04/02/bayesian-course.html) in Kelowna on November 20^th^-21^st^, 2014. They were written by [Dr. Joseph Thorley R.P.Bio.](http://www.poissonconsulting.ca/people/joe-thorley.html).

## Installation

If you haven't already done so, download the the most recent version of the R base distribution binary for your platform from <http://cran.r-project.org/> and install using the default options. Next download and install RStudio from <http://www.rstudio.com/products/rstudio/download/> using the default options. Then, download JAGS from <http://sourceforge.net/projects/mcmc-jags/files/JAGS/> and install with the default options. 

To make sure you have all the required packages execute
the following code at the command line
```{r, message = FALSE, warning = FALSE}
#install.packages("devtools", quiet = TRUE)
library(devtools)

#install.packages("dplyr", quiet = TRUE)
library(dplyr)

#install.packages("ggplot2", quiet = TRUE)
library(ggplot2)

#install.packages("rjags", quiet = TRUE)
#install_github("poissonconsulting/tulip@v0.0.11")
#install_github("poissonconsulting/datalist@v0.4")
#install_github("poissonconsulting/juggler@v0.1.3")

#install_github("poissonconsulting/jaggernaut@v1.8.5")
library(jaggernaut)
```

## Bayesian vs. Frequentist Statistical Analysis

Statistical analysis uses probability models 
to provide bounded estimates of parameter values 
($\theta$) from the data ($y$).

There are two primary approaches to statistical analysis: Bayesian and frequentist. As far as a frequentist is concerned the best estimates of $\theta$ are those values that maximise the *likelihood* which is the probability of the data given the estimates, i.e., $p(y|\theta)$. 
A Bayesian on the other hand chooses the values with the highest *posterior* probability - that is to say the probability of the estimates given the data , i.e., $p(\theta|y)$. 

```{r, echo = FALSE, results = 'asis'}
exercise("Which criterion makes the most sense to you? And why?")
```

There are several major consequences of this choice. 

One major consequence is that Bayesians need to provide a *prior* probability distributions for $\theta$ because (as **Bayes** theorem demonstrates) $P(\theta|y) \propto P(y|\theta)P(\theta)$. Bayesians consider
this an advantage because prior information can be incorporated into an analysis while frequentists consider it [subjective](http://dilbert.com/strips/comic/2008-05-08/). In most cases Bayesians use *low-information* priors which have negligible effect on the posteriors.

A second major consequence is that frequentist 95% *confidence intervals* (CIs) are expected to include the actual values of $\theta$ 95% of the time. Bayesian 95% *credible intervals* (CRIs), on the other hand, have a 95% probability of including the actual
values of $\theta$. The difference is subtle but important. For example, the
the reliability of frequentist confidence intervals depend on *sufficient*
sample size - Bayesian credible intervals do not.

A third major consequence is that although likelihood functions can be derived 
for many models (and therefore used to quickly find the maximum likelihood (ML) estimates),
it is typically not possible to to calculate the posterior probababilities.
As a result the posterior probabilities have to be sampled using Markov Chain Monte Carlo (MCMC) algorithms such as Gibbs Sampling.

## Gibbs Sampling

Consider the case where the parameters $\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ 
then Gibbs Sampling proceed as follows

Step 1

:   Choose starting *initial* values $\theta_1^{(0)}, \theta_2^{(0)}, \ldots, \theta_k^{(0)}$

Step 2

:   Sample $\theta_1^{(1)}$ from $p(\theta_1|\theta_2^{(0)},\theta_3^{(0)}, \ldots, \theta_k^{(0)}, y)$

    Sample $\theta_2^{(1)}$ from $p(\theta_2|\theta_1^{(1)},\theta_3^{(0)}, \ldots, \theta_k^{(0)}, y)$
    
    $\cdots$
    
    Sample $\theta_k^{(1)}$ from $p(\theta_k|\theta_1^{(1)},\theta_2^{(1)}, \ldots, \theta_{k-1}^{(1)}, y)$

Step 3

:   *Iterate* step 2 thousands (or millions) of times to obtain a sample from $p(\theta|y)$.

## JAGS and BUGS

Programming an efficient Gibbs Sampler for a particular model is outside the scope of 
most research projects. 
Fortunately, JAGS (which stands for Just Another Gibbs Sampler) can take a dataset
and a model specified in the simple but flexible BUGS language (which stands for Bayesian Analysis Using Gibbs Sampling) and perform iterative sampling for us. 

In order to do this we will use the `jaggernaut` package to talk to
the standalone JAGS program via the `rjags` package.

## Biased Coin

Now consider the example of a biased coin which has a 75% chance of giving a tail.
We can generate a dummy data set of 10 values 
using the R function `rbinom()` 
which provides 
[pseudo-random](http://search.dilbert.com/comic/Random%20Nine) 
samples from a binomial distribution.

```{r, fig.width=3, fig.height=3}
set.seed(563) # specifies start point for pseudo-random number generation
y <- rbinom(n = 10, size = 1, prob = 0.75) # generates vector of 1000 samples
y
mean(y) # calculates mean of vector
qplot(y, geom = "histogram") # plots histogram of vector
```

In order to estimate the underlying probability of throwing a head we specify the following model in the BUGS language and save it as an object of class `jags_model`.
```{r, tidy=FALSE}
mcoin1 <- jags_model("model { # start of BUGS model code
  theta ~ dunif(0, 1) # uniform prior distribution between 1 and 0 for theta
  for(i in 1:length(y)) { # for loop over the values 1, 2, ..., length of y
    y[i] ~ dbin(theta, 1) # assumed statistical distribution for observed throws
  } # end of for loop
} # end of BUGS model code
") 
```

```{r, echo = FALSE, results = 'asis'}
exercise("Plot theta's prior distribution as a histogram. You will likely find the `runif` and `qplot` functions useful.")
```

Then we call JAGS using `jaggernaut` in `debug` mode to generate 100 samples from 
$\theta$'s posterior probability distribution.
```{r, message=FALSE, warning=FALSE, results='hide'}
acoin1 <- jags_analysis(mcoin1, data = data.frame(y = y), mode = "debug")
```

```{r, fig.width=6, fig.height=3}
plot(acoin1)
coef(acoin1)
```

The model output indicates that the point estimate is `r round(coef(acoin1)[,"estimate"],2)`, the 95% credible interval (CRI) is `r round(coef(acoin1)[,"lower"],2)` to `r round(coef(acoin1)[,"upper"],2)`, the standard deviation is `r round(coef(acoin1)[,"sd"],2)`,
the percent relative error is `r coef(acoin1)[,"error"]` and the significance is `r coef(acoin1)[,"significance"]`.


If previous studies indicated that the coin was biased towards tails then we could adjust the prior distribution as follows.
```{r, tidy=FALSE}
mcoin2 <- jags_model("model { # start of BUGS model code
  theta ~ dunif(0.5, 1) # new prior distribution
  for(i in 1:length(y)) { 
    y[i] ~ dbin(theta, 1) 
  } 
} ")
```

```{r, echo = FALSE, results = 'asis'}
exercise("How does the new prior affect the posterior distribution?")
```

### Recap

- The BUGS model code defines the posterior distributions and relationships between
parameters.
- Based on the data and model definition, JAGS uses MCMC algorithms to iteratively sample from the posterior distributions of the monitored parameters.
- The posterior distributions can be summarised in terms of a point estimate and 95% credible intervals.


## Black Cherry Trees

The `trees` data set in the dataset package provides information on
the girth and volume of 31 black cherry trees.

```{r, fig.width=3, fig.height=3}
qplot(x = Girth, y = Volume, data = trees)
```

Algebraically, the linear regression of `Volume` against `Girth` can be defined as follows

$$Volume_{i}  = \alpha + \beta * Girth_{i} + \epsilon_{i}$$
  
  where $\alpha$ is the intercept and $\beta$ is the slope and the error terms ($\epsilon_{i}$) are  drawn from a normal distribution with an standard deviation of $\sigma$.


The model can be defined as follows in the BUGS language where
`<-` indicates a *deterministic* as 
opposed to *stochastic* node (which is indicated by `~`).

```{r, tidy=FALSE}
mtrees1 <- jags_model("model {
  alpha ~ dnorm(0, 50^-2) # normal prior on intercept
  beta ~ dnorm(0, 50^-2) # normal prior on slope
  sigma ~ dunif(0, 50) # positive uniform prior residual variation

  for(i in 1:length(Volume)) { 
    eVolume[i] <- alpha + beta * Girth[i] # expected volume
    Volume[i] ~ dnorm(eVolume[i], sigma^-2) # observed volume drawn from normal
  } 
}")
```

The standard deviations of the normal distributions are raised to the
power of `-2` because (for historical reasons) Bayesians quantify variation 
in terms of the *precision* ($\tau$) as opposed to the variance ($\sigma^2$) or standard deviation ($\sigma$) where $\tau = 1/\sigma^2$.

The resultant trace plots and coefficients are as follows
```{r, message=FALSE, warning=FALSE, results='hide'}
atrees1 <- jags_analysis(mtrees1, data = datasets::trees, mode = "debug")
```

```{r, fig.width = 6, fig.height = 6}
plot(atrees1)
coef(atrees1)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What do the trace plots suggest to you?")
```

The $\hat{R}$ metric uses the within-chain and between-chain variances
to quantify their similarities with a value of `1.0` indicative of
convergence. For reports an $\hat{R} < 1.1$ is considered sufficient
while papers typically demand an $\hat{R} < 1.05$.
```{r}
rhat(atrees1, parm = "fixed", combine = FALSE)
```

Lack of convergence suggests that the MCMC samples may not be representative 
of the posterior distributions. To produce more representative samples,
the user can attempt to reparameterise the model to increase chain mixing
and/or increase the number of iterations.

### Chain Mixing

Poor chain mixing/sampling efficiency is often caused by cross correlations
between parameters. Examination of the residual plots indicates
that `alpha` and `beta` are cross correlated. The following code uses
the `select` argument to specify the required variables in the input
data with the `+` suffix on `Girth` indicating that it is to be
centered (`x - mean(x)`) before being passed to JAGS. 

```{r, tidy=FALSE}
mtrees2 <- jags_model("model {
  alpha ~ dnorm(0, 50^-2) # normal prior on intercept
  beta ~ dnorm(0, 50^-2) # normal prior on slope
  sigma ~ dunif(0, 50) # positive uniform prior residual variation

  for(i in 1:length(Volume)) { 
    eVolume[i] <- alpha + beta * Girth[i] # expected volume
    Volume[i] ~ dnorm(eVolume[i], sigma^-2) # observed volume drawn from normal
  } 
}",
select = c("Volume", "Girth+"))
```


```{r, echo = FALSE, results = 'asis'}
exercise("How does centering `Girth` change the trace plots and rhat values?")
```

### Iterations

Convergence can be achieved by increasing the number of
iterations and discarding the initial burnin period.
This is easy to achieve using jaggernaut, just
switch to `report` mode and set `niters = 10^4`

```{r, message=FALSE, warning=FALSE, results='hide'}
opts_jagr(mode = "report")
atrees3 <- jags_analysis(mtrees1, data = datasets::trees, niters = 10^4)
```

With these settings `jaggernaut` 

- conducts adaptive phase to maximize sampling efficiency
- generates three chains of 10^4 iterations in length.
- discards the first half of each chain as burnin
- thins each chain so that the total number of MCMC samples
for each monitored parameter is as close as possible to but not less than 10^3
- tests for convergence under the criterion that $\hat{R} < 1.1$
- if convergence has not been achieved it discards the current
samples as burnin and doubles the number of iterations before
thinning again
- it repeats this up to three times or until convergence is achieved

For more information on the possible modes and options type `?opts_jagr`.

```{r, echo = FALSE, results = 'asis'}
exercise("How does switching to report mode with 10^3 iterations
         affect the trace plots with and without centering Girth?")
```
