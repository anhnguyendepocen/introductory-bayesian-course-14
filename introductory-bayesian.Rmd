---
title: "Introductory Bayesian Course"
author: "Dr. Joseph L. Thorley"
date: "September 18^th^, 2014"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    keep_md: yes
    toc: yes
---

```{r,echo = FALSE, results='asis'}
library(knitr)
opts_chunk$set(tidy = TRUE)
options(width = 70)

exercise_number <- 0
exercise <- function (txt) {
  exercise_number <<- exercise_number + 1
  cat(paste0("\n**Exercise** ", exercise_number), paste0("*", txt, "*\n"))
}

ktable <- function (...) {
  x <- c(...)
  mat <- matrix(NA, nrow = length(x), ncol = 2)
  mat[, 1] <- names(x)
  mat[, 2] <- x
  colnames(mat) <- mat[1,]
  mat <- mat[-1, , drop = FALSE]
  mat <- mat[order(mat[, 1, drop = TRUE]), , drop = FALSE]
  cat("\n")
  cat(kable(mat, row.names = FALSE, output = FALSE), sep = "\n")
  cat("\n")
}

options(repos = c("CRAN" = "http://cran.rstudio.com"))

fig_width <- 4
fig_height <- fig_width

fn = local({
  i <- 0
  function(x) {
    i <<- i + 1
    paste0("Figure ", i, ": ", x)
  }
})

```

# Background

The purpose of these course notes is to introduce participants to Bayesian analysis with R, RStudio and JAGS. It is assumed that participants are familiar with R and RStudio as covered
in the Introductory R Course notes at <http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html>.

## Licence

The notes, which are released under a [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/deed.en_US) license, are a draft of the material to be presented at the [Introductory Bayesian Course](http://www.poissonconsulting.ca/courses/2014/04/02/bayesian-course.html) in Kelowna on November 20^th^-21^st^, 2014. They were written by [Dr. Joseph Thorley R.P.Bio.](http://www.poissonconsulting.ca/people/joe-thorley.html).

## Installation

If you haven't already done so, download the the most recent version of the R base distribution binary for your platform from <http://cran.r-project.org/> and install using the default options. Next download and install RStudio from <http://www.rstudio.com/products/rstudio/download/> using the default options. Then, download JAGS from <http://sourceforge.net/projects/mcmc-jags/files/JAGS/> and install with the default options. 

To make sure you have all the required packages execute
the following code at the command line
```{r, message = FALSE}

install.packages("devtools")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("rjags")

library(devtools)

install_github("poissonconsulting/tulip@v0.0.11")
install_github("poissonconsulting/datalist@v0.4")
install_github("poissonconsulting/juggler@v0.1.3")
install_github("poissonconsulting/jaggernaut@v1.8.5")

library(dplyr)
library(ggplot2)
library(jaggernaut)
```

## Bayesian vs. Frequentist Statistical Analysis

Statistical analysis uses probability models 
to provide bounded estimates of parameter values 
($\theta$) from the data ($y$).

There are two primary approaches to statistical analysis: Bayesian and frequentist. As far as a frequentist is concerned the best estimates of $\theta$ are those values that maximise the *likelihood* which is the probability of the data given the estimates, i.e., $p(y|\theta)$. 
A Bayesian on the other hand chooses the values with the highest *posterior* probability - that is to say the probability of the estimates given the data , i.e., $p(\theta|y)$. 

```{r, echo = FALSE, results = 'asis'}
exercise("Which criterion makes the most sense to you? And why?")
```

There are several major consequences of this choice. 

One major consequence is that Bayesians need to provide a *prior* probability distributions for $\theta$ because (as **Bayes** theorem demonstrates) $P(\theta|y) \propto P(y|\theta)P(\theta)$. Bayesians consider
this an advantage because prior information can be incorporated into an analysis while frequentists consider it [subjective](http://dilbert.com/strips/comic/2008-05-08/). In most cases Bayesians use *low-information* priors which have negligible effect on the posteriors.

A second major consequence is that frequentist 95% *confidence intervals* (CIs) are expected to include the actual values of $\theta$ 95% of the time. Bayesian 95% *credible intervals* (CRIs), on the other hand, have a 95% probability of including the actual
values of $\theta$. The difference is subtle but important. For example, the
the reliability of frequentist confidence intervals depend on *sufficient*
sample size - Bayesian credible intervals do not.

A third major consequence is that although likelihood functions can be derived 
for many models (and therefore used to quickly find the maximum likelihood (ML) estimates),
it is typically not possible to to calculate the posterior probababilities.
As a result the posterior probabilities have to be sampled using Markov Chain Monte Carlo (MCMC) algorithms such as Gibbs Sampling.

## Gibbs Sampling

Consider the case where the parameters $\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ 
then Gibbs Sampling proceed as follows

Step 1

:   Choose starting *initial* values $\theta_1^{(0)}, \theta_2^{(0)}, \ldots, \theta_k^{(0)}$

Step 2

:   Sample $\theta_1^{(1)}$ from $p(\theta_1|\theta_2^{(0)},\theta_3^{(0)}, \ldots, \theta_k^{(0)}, y)$

    Sample $\theta_2^{(1)}$ from $p(\theta_2|\theta_1^{(1)},\theta_3^{(0)}, \ldots, \theta_k^{(0)}, y)$
    
    $\cdots$
    
    Sample $\theta_k^{(1)}$ from $p(\theta_k|\theta_1^{(1)},\theta_2^{(1)}, \ldots, \theta_{k-1}^{(1)}, y)$

Step 3

:   *Iterate* step 2 thousands (or millions) of times to obtain a sample from $p(\theta|y)$.

## JAGS and BUGS

Programming an efficient Gibbs Sampler for a particular model is outside the scope of 
most research projects. 
Fortunately, JAGS (which stands for Just Another Gibbs Sampler) can take a dataset
and a model specified in the simple but flexible BUGS language (which stands for Bayesian Analysis Using Gibbs Sampling) and perform iterative sampling for us. 

In order to do this we use the `jaggernaut` package to talk to
the standalone JAGS program via the `rjags` package.

## Biased Coin

Now consider the example of a biased coin which has a 75% chance of giving a tail.
We can generate a dummy data set of 10 values 
using the R function `rbinom()` 
which provides 
[pseudo-random](http://search.dilbert.com/comic/Random%20Nine) 
samples from a binomial distribution.

```{r, fig.width=3, fig.height=3}
set.seed(563) # specifies start point for pseudo-random number generation
y <- rbinom(n = 10, size = 1, prob = 0.75) # generates vector of 1000 samples
y
mean(y) # calculates mean of vector
qplot(y, geom = "histogram") # plots histogram of vector
```

```{r, echo = FALSE, results = 'asis'}
exercise("Create a new R project in a folder on your hard drive called `Bayes`")
exercise("Create a project script called `coin.R` to generate 
  the above ten throws of a coin with a 75% chance of a tail.")
```

In order to estimate the underlying probability of throwing a head we need to specify the following model in the BUGS language and save it as an object of class `jags_model`.
```{r, tidy=FALSE}
mcoin1 <- jags_model("model { # start of BUGS model code
  theta ~ dunif(0, 1) # uniform prior distribution between 1 and 0 for theta
  for(i in 1:length(y)) { # for loop over the values 1, 2, ..., length of y
    y[i] ~ dbin(theta, 1) # assumed statistical distribution for observed throws
  } # end of for loop
} # end of BUGS model code") 
```

```{r, echo = FALSE, results = 'asis'}
exercise("Plot theta's prior distribution as a histogram. You will likely find the `runif` and `hist` functions useful.")
```

Then we call JAGS using jaggernaut to generate 100 samples from 
$\theta$'s posterior probability distribution.
```{r, fig.width=6, fig.height=3}
mcoin1 <- jags_analysis("coin.jags", data = data.frame(y = y))
pcoin1 <- coda.samples(mcoin1, variable.names = "theta", n.iter = 100)
plot(pcoin1)
summary(pcoin1)
```

Any questions?

```{r, echo = FALSE, results = 'asis'}
exercise("What is the point estimate for theta?")
exercise("And the 95% credible interval?")
```

The same model can be fitted in a frequentist framework using the following R code
```{r}
mod <- glm(y ~ 1, data = list(y = y), family = "binomial")
plogis(coef(mod))
plogis(confint(mod))
```

The following BUGS model code includes a derived parameter `range` which 
test the probability that theta lies between 0.5 and 0.7.
```{r, tidy=FALSE}
txt <- "model { # start of BUGS model code
  theta ~ dunif(0, 1) # uniform prior distribution between 1 and 0 for theta
  for(i in 1:length(y)) { # for loop over the values 1, 2, ..., length of y
    y[i] ~ dbin(theta, 1) # assumed statistical distribution for observed throws
  } # end of for loop
  range <- theta >= 0.5 && theta <= 0.7
} # end of BUGS model code" 
writeLines(txt, "coin.jags")
```

```{r, echo = FALSE, results = 'asis'}
exercise("What is the probability that the coin can be expected to produce a tail between 50 and 70% of the time?")
exercise("How would you answer the above question in the frequentist framework?")
exercise("What is the probability that the coin can be expected to produce a tail more than 50% of the time?")
```

If previous studies indicated that the coin was definitely biased towards tails then we could adjust the prior distribution as follows.
```{r, tidy=FALSE}
txt <- "model { # start of BUGS model code
  theta ~ dunif(0.5, 1) # uniform prior distribution between 1 and 0 for theta
  for(i in 1:length(y)) { # for loop over the values 1, 2, ..., length of y
    y[i] ~ dbin(theta, 1) # assumed statistical distribution for observed throws
  } # end of for loop
} # end of BUGS model code" 
writeLines(txt, "coin.jags")
```

```{r, echo = FALSE, results = 'asis'}
exercise("What does the new prior distribution look like?")
exercise("How does it affect the posterior distribution?")
```

## Fish Stranding Events

Imagine a dam operator comes to you with the following data regarding 
fish stranding events. Historically fish stranding occurred 15% of the time.
```{r}
y <- c(0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 
0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 
0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 
0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0)
```

```{r, echo = FALSE, results = 'asis'}
exercise("What is the probability that dam operations have increased the frequency of fish stranding?")
exercise("How does the inter-run stability of your answer depend on the number of iterations (`n.iter` in `coda.samples` specifies the number of iterations)?")
```

### Recap

- JAGS is a stand-alone program
- A package such as `rjags` is required to allow R to talk to JAGS
- `rjags` requires the data in the form of a list
- The BUGS model code defines the posterior distributions and relationships between
parameters
- Based on the data and model definition, JAGS uses MCMC algorithms to iteratively sample from the posterior distributions of the monitored parameters
- The posterior distributions can be summarised in terms of a point estimate and 95% credible intervals

## Black Cherry Trees

The `trees` data set in the dataset package provides information on
girth, height and volume of 31 black cherry trees.

```{r, fig.width=3, fig.height=3}
qplot(x = Girth, y = Volume, data = trees)
```

Algebraically, the linear regression of `Volume` against `Girth` can be defined as follows

$$Volume_{i}  = \alpha + \beta * Girth_{i} + \epsilon_{i}$$

where $\alpha$ is the intercept and $\beta$ is the slope and the error terms ($\epsilon_{i}$) are  drawn from a normal distribution with an standard deviation of $\sigma$.

In the frequentist framework it can be defined and fitted as follows
```{r, tidy=FALSE}
mod <- lm(Volume ~ Girth, data = trees)
summary(mod)
```

and plotted using

```{r, fig.width=3, fig.height=3}
newdata <- datalist::new_data(trees, "Girth")
pred <- predict(mod, newdata, interval = "conf")
newdata <- cbind(newdata, data.frame(pred))

gp <- ggplot(data = newdata, aes(x = Girth, y = fit))
gp <- gp + geom_point(data = trees, aes(y = Volume))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lwr), linetype = "dashed")
gp <- gp + geom_line(aes(y = upr), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Volume")

print(gp)
```

The model can be defined as follows in the BUGS language where
`<-` indicates a *deterministic* as opposed to *stochastic* node (which is indicated by `~`).
```{r, tidy=FALSE}
txt <- "model {
  alpha ~ dnorm(0, 50^-2) # normal prior on intercept
  beta ~ dnorm(0, 50^-2) # normal prior on slope
  sigma ~ dunif(0, 50) # positive uniform prior residual variation

  for(i in 1:length(Volume)) { 
    eVolume[i] <- alpha + beta * Girth[i] # expected volume
    Volume[i] ~ dnorm(eVolume[i], sigma^-2) # observed volume drawn from normal
  } 
}" 
writeLines(txt, "trees.jags")
```

The standard deviations of the normal distributions are raised to the
power of `-2` because (for historical reasons) Bayesians quantify variation 
in terms of the *precision* ($\tau$) as opposed to the variance ($\sigma^2$) or standard deviation ($\sigma$) where $\tau = 1/\sigma^2$.

```{r, echo = FALSE, results = 'asis'}
exercise("What do you notice about the prior distribution used for `sigma` versus the other two parameters? Why do you think this is?")
```

As before the posteriors can be sampled using the `rjags` function `coda.samples`.
```{r, warning = FALSE, message = FALSE, fig.width=6, fig.height=6}
mtrees1 <- jags.model("trees.jags", data = as.list(datasets::trees), quiet = TRUE)
ptrees1 <- coda.samples(mtrees1, variable.names = c("alpha", "beta", "sigma"), n.iter = 1000)
plot(ptrees1)
```

Notice how the iterations begin at 1000. This is because the model requires 
a thousand iterations to complete the *adaptive phase*. As it states in the `rjags` help files 

> During the adaptive phase the samplers adapt their behaviour to maximize their efficiency... The sequence of samples generated during this adaptive phase is not a Markov chain, and therefore may not be used for posterior inference on the model.

```{r, echo = FALSE, results = 'asis'}
exercise("What do you notice about the trace plots for `alpha` and `beta`?And why do you think this is the case?")
```

Any autocorrelation in the traceplots can be quantified using the
`coda` function `autocorr.diag`

```{r}
autocorr.diag(ptrees1)
```

Researchers sometimes reduce the autocorrelation
by thinning the chains. However as argued and demonstrated by xx, although high levels
of autocorrelation may require longer chains thinning per se is never justified (although longer chains may require thinning to ensure they
remain manageable).

It is worth noting that high levels of autocorrelation is often caused by cross correlations between parameters
```{r}
crosscorr(ptrees1)
```
which can often be eliminated by reparameterising the model. 

Below we attempt to remove the cross correlation between
the intercept (`alpha`) and the slope (`beta`)
by *centering* `Girth`
```{r, warning = FALSE, message = FALSE}
trees <- dplyr::mutate(datasets::trees, Girth = Girth - mean(Girth))
mtrees2 <- jags.model("trees.jags", data = as.list(trees), quiet = TRUE)
ptrees2 <- coda.samples(mtrees2, variable.names = c("alpha", "beta", "sigma"), n.iter = 1000)
```

```{r, echo = FALSE, results = 'asis'}
exercise("Does centering `Girth` eliminate the cross correlation?")
exercise("What is the effect of centering `Girth` on the
         traceplots and autocorrelations?")
```

Many researchers estimate fitted values, predictions and residuals by adding extra monitored nodes to their model code. The disadvantages of
this approach are that: the model code becomes more complicated; 
the MCMC sampling takes longer and the table of parameter estimates
becomes unwiedly. Plus, if the variables have been transformed 
back-transformation of the values is also required.

To circumvent these problems I wrote the R package `jaggernaut`
which acts as a wrapper on `rjags`. The following code implements
the previous model in jaggernaut and allows the subsequent generation 
of predictions, fitted values and residuals.

```{r, message = FALSE, warning=FALSE}
mtrees3 <- jags_model("model {
  alpha ~ dnorm(0, 50^-2) # normal prior on intercept
  beta ~ dnorm(0, 50^-2) # normal prior on slope
  sigma ~ dunif(0, 50) # positive uniform prior residual variation
  
  for(i in 1:length(Volume)) { 
    eVolume[i] <- alpha + beta * Girth[i] # expected volume
    Volume[i] ~ dnorm(eVolume[i], sigma^-2) # observed volume drawn from normal
    } 
  }",
derived_code = "data {
  for(i in 1:length(Volume)) { 
    prediction[i] <- alpha + beta * Girth[i]
  }
  residual <- (Volume - prediction) / sigma
}
",
monitor = c("alpha", "beta", "sigma"),
select = c("Volume", "Girth+"))
```

As their names imply the `monitor` and `select` arguments
specify the parameters to monitor and the variables to select
from the input dataset. The `+` after `Girth` in the `select`
argument indicates that Girth is to be back-transformed by 
subtracting its mean

```{r, message = FALSE, warning=FALSE}
atrees3 <- jags_analysis(mtrees3, data = datasets::trees)
summary(atrees3)

prediction <- predict(atrees3, newdata = "Girth")
residuals <- residuals(atrees3)
fitted <- fitted(atrees3)$estimate
```

```{r, fig.width=3, fig.height=3}
gp <- ggplot(data = prediction, aes(x = Girth, y = estimate))
gp <- gp + geom_point(data = datasets::trees, aes(y = Volume))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Volume")

print(gp)

qplot(fitted$estimate, residuals$estimate) + geom_hline(yintercept = 0) + geom_smooth(se = FALSE)
```

As discussed in the R course [notes](http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html) the relationship between Volume and Girth is expected to be [allometric](http://www.nature.com/scitable/knowledge/library/allometry-the-study-of-biological-scaling-13228439) because the cross-sectional area at an given point scales to the square of the girth (circumference).

```{r, echo = FALSE, results = 'asis'}
exercise("Fit an allometric relationship by log What is the probability that dam operations have increased the frequency of fish stranding?")
```

Exercise 36 Try fitting an allometric relationship by log transforming Volume and Girth and then using the same basic linear regression model. Does the diagnostic plot suggest an improvement in the model?

