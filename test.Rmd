---
title: "Introductory Bayesian Course"
author: "Dr. Joseph L. Thorley"
date: "October 20^th^, 2014"
output:
  pdf_document:
    number_sections: yes
    toc: yes
  html_document:
    keep_md: yes
    toc: yes
---

```{r,echo = FALSE, results='asis'}
library(knitr)
opts_chunk$set(tidy = TRUE)
options(width = 70)

exercise_number <- 0
exercise <- function (txt) {
  exercise_number <<- exercise_number + 1
  cat(paste0("\n**Exercise** ", exercise_number), paste0("*", txt, "*\n"))
}

ktable <- function (...) {
  x <- c(...)
  mat <- matrix(NA, nrow = length(x), ncol = 2)
  mat[, 1] <- names(x)
  mat[, 2] <- x
  colnames(mat) <- mat[1,]
  mat <- mat[-1, , drop = FALSE]
  mat <- mat[order(mat[, 1, drop = TRUE]), , drop = FALSE]
  cat("\n")
  cat(kable(mat, row.names = FALSE, output = FALSE), sep = "\n")
  cat("\n")
}

options(repos = c("CRAN" = "http://cran.rstudio.com"))

fig_width <- 4
fig_height <- fig_width

fn = local({
  i <- 0
  function(x) {
    i <<- i + 1
    paste0("Figure ", i, ": ", x)
  }
})

```

# Background

The purpose of these course notes is to introduce participants to Bayesian analysis with R, RStudio and JAGS. It is assumed that participants are familiar with R and RStudio as covered
in the Introductory R Course notes at <http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html>.

## Licence

The notes, which are released under a [CC BY 4.0](http://creativecommons.org/licenses/by/4.0/deed.en_US) license, are a draft of the material to be presented at the [Introductory Bayesian Course](http://www.poissonconsulting.ca/courses/2014/04/02/bayesian-course.html) in Kelowna on November 20^th^-21^st^, 2014. They were written by [Dr. Joseph Thorley R.P.Bio.](http://www.poissonconsulting.ca/people/joe-thorley.html).

## Installation

If you haven't already done so, download the the most recent version of the R base distribution binary for your platform from <http://cran.r-project.org/> and install using the default options. Next download and install RStudio from <http://www.rstudio.com/products/rstudio/download/> using the default options. Then, download JAGS from <http://sourceforge.net/projects/mcmc-jags/files/JAGS/> and install with the default options. 

To make sure you have all the required packages installed on your hard drive execute
the following code at the command line

```
install.packages("devtools", quiet = TRUE)
library(devtools)

install.packages("dplyr", quiet = TRUE)

install.packages("ggplot2", quiet = TRUE)
install.packages("scales", quiet = TRUE)

install_github("poissonconsulting/tulip@v0.0.11")
install_github("poissonconsulting/datalist@v0.4")
install_github("poissonconsulting/juggler@v0.1.3")
install_github("poissonconsulting/jaggernaut@v2.0.0")
```

Then start any scripts with 

```{r, message=FALSE}
library(dplyr)
library(ggplot2)
library(scales)
library(jaggernaut)
```

## Bayesian and Frequentist Statistical Analysis

Statistical analysis uses probability models 
to provide bounded estimates of parameter values 
($\theta$) from the data ($y$).

There are two primary approaches to statistical analysis: Bayesian and frequentist. As far as a frequentist is concerned the best estimates of $\theta$ are those values that maximise the *likelihood* which is the probability of the data given the estimates, i.e., $p(y|\theta)$. 
A Bayesian on the other hand chooses the values with the highest *posterior* probability - that is to say the probability of the estimates given the data , i.e., $p(\theta|y)$.

### Coin Flips

Consider the case where $n = 10$ flips of a coin produce $y = 3$ tails.
We can model this using a binomial distribution

$$y \sim dbin(\theta, n)$$

where $\theta$ is the probability of throwing a head.

#### Maximum Likelihood

The likelihood for the binomial model is given by the following equation

$$ p(y|\theta) =  {n \choose y} \theta^{y}(1-\theta)^{n-y}$$.

The likelihood values for different values of $\theta$ are
therefore as follows

```{r, fig.width = 3, fig.height = 3}
likelihood <- function (theta, n = 10, y = 3) {
  choose(n, y) * theta^y * (1 - theta)^(n-y)
}
theta <- seq(from = 0, to = 1, length.out = 100)

qplot(theta, likelihood(theta), geom = "line", ylab = "Likelihood", xlab = "theta")
```

The estimate ($\hat{\theta}$) is the value of $\theta$ with the maximum likelihood (ML) value, which in this case is 0.3.

A 95% confidence interval (CI) can then be calculated using the asymptotic normal approximation

$$\hat{\theta} \pm 1.96 \frac{1}{\sqrt{I(\hat{\theta})}}$$

where $I(\hat{\theta})$ is the expected second derivative 
of the log-likelihood at the estimate. This calculation is 
based on the assumption that the sample size is of sufficient
size that the likelihood is normally distributed.

In the current case, 

$$I(\hat{\theta}) = \frac{n}{\hat{\theta}(1 - \hat{\theta})}$$

which gives a point estimate of 0.3 and lower and upper
95% confidence intervals of `r round(0.3 - 1.96 * (1/sqrt(10/0.21)),2)`
and `r round(0.3 + 1.96 * (1/sqrt(10/0.21)),2)` respectively.

#### Posterior Probability

The posterior probability on the other hand 
is given by Bayes rule which states that

$$p(\theta|y) \propto p(y|\theta)p(\theta)$$

where $p(\theta)$ is the prior probability.

Bayesians consider this an advantage because prior information can be incorporated into an analysis while frequentists consider it [subjective](http://dilbert.com/strips/comic/2008-05-08/). In most cases Bayesians use *low-information* priors which have very small effects on the posteriors. For example a uniform distribution with a lower limit of 0 and an upper limit of 1 ($dunif(0,1)$) is commonly used for probabilities.

```{r, fig.width = 3, fig.height = 3}
qplot(runif(10^6, 0, 1), geom = "density", xlab = "theta")
```

As it is generally not possible to calculate the 
posterior probability, the  posterior probability distribution is sampled using Markov Chain Monte Carlo (MCMC) algorithms such as Gibbs Sampling.

##### Gibbs Sampling

Consider the case where the parameters 
$\theta = (\theta_1, \theta_2, \ldots, \theta_k)$ 
then Gibbs Sampling proceed as follows

Step 1

:   Choose starting *initial* values for $\theta_1^{(0)}$ and $\theta_2^{(0)}$

Step 2

:   Sample $\theta_1^{(1)}$ from $p(\theta_1|\theta_2^{(0)}, y)$

    Sample $\theta_2^{(1)}$ from $p(\theta_2|\theta_1^{(1)}, y)$

Step 3

:   *Iterate* step 2 thousands (or millions) of times to obtain a sample from $p(\theta|y)$.

## JAGS and BUGS

Programming an efficient MCMC algorithm
for a particular model is outside the scope of 
most research projects. 
Fortunately, JAGS (which stands for Just Another Gibbs Sampler) can take a dataset
and a model specified in the simple but flexible BUGS language (which stands for Bayesian Analysis Using Gibbs Sampling) and perform MCMC sampling for us. 

In order to do this we will use the `jaggernaut` package to talk to
the standalone JAGS program via the `rjags` package.

First we need to specify the underlying probability model in the BUGS language and save it as an object of class `jags_model`.
```{r, tidy=FALSE}
model1 <- jags_model("model { 
  theta ~ dunif(0, 1)
  y ~ dbin(theta, n)
}") 
```

then we call JAGS using `jaggernaut` in the default report mode 
to generate samples from $\theta$'s posterior probability distribution.

```{r, message=FALSE, warning=FALSE, results='hide'}
data <- data.frame(n = 10, y = 3)
analysis1 <- jags_analysis(model1, data = data)
```

```{r, fig.width=6, fig.height=3}
plot(analysis1)
coef(analysis1)
```

The model output indicates that the point estimate (in this case the
mean of the samples) is `r round(coef(analysis1)[,"estimate"],2)`
and the 95% credible interval (in this case the 2.25th and 97.75th 
percentiles) is `r round(coef(analysis1)[,"lower"],2)` to `r round(coef(analysis1)[,"upper"],2)`.

```{r, echo = FALSE, results = 'asis'}
exercise("Previous studies indicate that the coin was definitely biased
         towards tails. Modify the prior distribution accordingly
         and rerun the above model. How does the posterior distribution change?")
```

## Black Cherry Trees

The `trees` data set in the dataset package provides information on
the girth and volume of 31 black cherry trees.

```{r, fig.width=3, fig.height=3}
qplot(x = Girth, y = Volume, data = trees)
```

Algebraically, the linear regression of `Volume` against `Girth` can be defined as follows

$$Volume_{i}  = \alpha + \beta * Girth_{i} + \epsilon_{i}$$
  
  where $\alpha$ is the intercept and $\beta$ is the slope and the error terms ($\epsilon_{i}$) are  drawn from a normal distribution with an standard deviation of $\sigma$.


The model can be defined as follows in the BUGS language where
`<-` indicates a *deterministic* as 
opposed to *stochastic* node (which is indicated by `~`).

```{r, tidy=FALSE}
model1 <- jags_model("model {
  alpha ~ dnorm(0, 50^-2) 
  beta ~ dnorm(0, 10^-2)
  sigma ~ dunif(0, 10)

  for(i in 1:length(Volume)) { 
    eMu[i] <- alpha + beta * Girth[i]
    Volume[i] ~ dnorm(eMu[i], sigma^-2)
  } 
}")
```

The standard deviations of the normal distributions are raised to the
power of `-2` because (for historical reasons) Bayesians quantify variation 
in terms of the *precision* ($\tau$) as opposed to the variance ($\sigma^2$) or standard deviation ($\sigma$) where $\tau = 1/\sigma^2$.

The resultant trace plots and coefficients are as follows
```{r, message=FALSE, warning=FALSE, results='hide'}
data(trees)
analysis1 <- jags_analysis(model1, data = trees)
```

```{r, fig.width = 6, fig.height = 6}
plot(analysis1)
coef(analysis1)
```

### Predictions and Residuals

Many researchers estimate fitted values, 
predictions and residuals by monitoring additional nodes in their model code. The disadvantages of
this approach are that: 

- the model code becomes more complicated. 
- the MCMC sampling takes longer.
- adding derived parameters requires a model rerun.
- the table of parameter estimates becomes unwiedly.

`jaggernaut` overcomes these problems by allowing 
derived parameters to be defined
in a separate chunk of BUGS code as demonstrated below.

```{r, tidy=FALSE, message=FALSE, warning=FALSE, results='hide'}
dcode <- "data {
  for(i in 1:length(Volume)) { 
    prediction[i] <- alpha + beta * Girth[i]
  }
  residual <- (Volume - prediction) / sigma
}"

prediction <- predict(analysis1, newdata = "Girth", derived_code = dcode)
```

```{r, fig.width=3, fig.height=3, ,message=FALSE, warning=FALSE}
gp <- ggplot(data = prediction, aes(x = Girth, y = estimate))
gp <- gp + geom_point(data = dataset(analysis1), aes(y = Volume))
gp <- gp + geom_line()
gp <- gp + geom_line(aes(y = lower), linetype = "dashed")
gp <- gp + geom_line(aes(y = upper), linetype = "dashed")
gp <- gp + scale_y_continuous(name = "Volume")

print(gp)

fitted <- fitted(analysis1, derived_code = dcode)
fitted$residual <- residuals(analysis1, derived_code = dcode)$estimate

qplot(estimate, residual, data = fitted) + geom_hline(yintercept = 0) + geom_smooth(se = FALSE)
```

As discussed in the R course [notes](http://www.poissonconsulting.ca/course/2014/09/12/an-introduction-to-r-course.html) the relationship between Volume and Girth is expected to be [allometric](http://www.nature.com/scitable/knowledge/library/allometry-the-study-of-biological-scaling-13228439) because the cross-sectional area at an given point scales to the square of the girth (circumference).

Expressed as an allometric relationship the model becomes

$$Volume_{i}  = \alpha * Girth_{i}^\beta * \epsilon_{i}$$

which can be reparameterised as a linear regression by log transforming

$$log(Volume_{i})  = \alpha + \beta * log(Girth_{i}) + \epsilon_{i}$$

